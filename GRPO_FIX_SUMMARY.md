# GRPO Implementation Fix Summary

## ğŸ”´ åŸå§‹é—®é¢˜

ä½ å‘ç°äº†ä¸¤ä¸ª**è‡´å‘½ç¼ºé™·**ï¼Œä¼šå¯¼è‡´è®­ç»ƒè™½ç„¶èƒ½è·‘ä½†æ— æ³•å­¦åˆ°ä¸œè¥¿ï¼š

### é—®é¢˜ 1ï¼šæ¢¯åº¦ä¸¢å¤±
**åŸå› **ï¼š
- åœ¨ `torch.no_grad()` ä¸­ç”Ÿæˆè¾“å‡º
- ä» `generated.scores` ç›´æ¥è®¡ç®— log_probs
- è¿™äº› log_probs **ä¸å¸¦æ¢¯åº¦**
- loss æ— æ³•åå‘ä¼ æ’­åˆ° LoRA å‚æ•°

**åæœ**ï¼šLoRA å‚æ•°ä¸ä¼šæ›´æ–°ï¼Œæ¨¡å‹ä¸ä¼šå­¦ä¹ 

### é—®é¢˜ 2ï¼šVLæ¨¡å‹çœ‹ä¸åˆ°å›¾åƒ
**åŸå› **ï¼š
- `_sample_outputs` åªæ¥æ”¶æ–‡æœ¬ prompt
- æ²¡æœ‰ä¼ å…¥å›¾åƒæ•°æ®
- Qwen2.5-VL æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œ**å¿…é¡»åŒæ—¶æ¥æ”¶å›¾åƒå’Œæ–‡æœ¬**

**åæœ**ï¼šæ¨¡å‹åœ¨ç›²æ‰“ï¼Œæ— æ³•æ ¹æ®å›¾åƒå†…å®¹é¢„æµ‹è´Ÿç‚¹

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### æ­£ç¡®çš„ GRPO æµç¨‹

```
é˜¶æ®µ1ï¼šé‡‡æ ·ï¼ˆno_gradï¼‰
  â””â”€ ç”Ÿæˆ K ä¸ªå€™é€‰è¾“å‡ºï¼ˆåªç”¨äºè·å–æ–‡æœ¬ï¼‰

é˜¶æ®µ2ï¼šå¥–åŠ±è®¡ç®—
  â””â”€ ç”¨ SAM2 è¯„ä¼°æ¯ä¸ªå€™é€‰çš„è´¨é‡

é˜¶æ®µ3ï¼šé‡æ–°è®¡ç®— log_probsï¼ˆå¸¦æ¢¯åº¦ï¼‰
  â””â”€ å‰å‘ä¼ æ’­è®¡ç®—å¸¦æ¢¯åº¦çš„ log_probs

é˜¶æ®µ4ï¼šå‚æ•°æ›´æ–°
  â””â”€ ç”¨ GRPO loss æ›´æ–° LoRA
```

### ä¿®å¤ 1ï¼šé‡æ–°è®¡ç®—å¸¦æ¢¯åº¦çš„ log_probs

**ä¿®æ”¹å‰**ï¼š
```python
def _sample_outputs(self, prompt: str, k: int):
    for _ in range(k):
        with torch.no_grad():  # âŒ æ²¡æœ‰æ¢¯åº¦
            generated = model.generate(...)

        # âŒ ç›´æ¥ä» generated.scores è®¡ç®—ï¼Œæ²¡æœ‰æ¢¯åº¦
        log_prob = self._compute_log_prob(generated)
        log_probs.append(log_prob)

    return outputs, log_probs  # âŒ log_probs æ— æ¢¯åº¦
```

**ä¿®æ”¹å**ï¼š
```python
def _sample_outputs(self, inputs: Dict, k: int):
    for _ in range(k):
        with torch.no_grad():  # âœ… é‡‡æ ·æ—¶ä¸éœ€è¦æ¢¯åº¦
            generated = model.generate(...)

        # âœ… åªä¿å­˜ç”Ÿæˆçš„ token åºåˆ—
        sequences.append(seq)

    return outputs_text, sequences  # âœ… è¿”å›åºåˆ—ï¼Œä¸è¿”å›log_probs

def _compute_sequence_log_probs(self, inputs, sequence):
    """âœ… é‡æ–°å‰å‘ä¼ æ’­ï¼Œè®¡ç®—å¸¦æ¢¯åº¦çš„ log_probs"""

    # âœ… å‰å‘ä¼ æ’­ï¼ˆå¸¦æ¢¯åº¦ï¼‰
    outputs = model(**forward_inputs)
    logits = outputs.logits

    # âœ… è®¡ç®— log_probsï¼ˆå¸¦æ¢¯åº¦ï¼‰
    log_probs = torch.log_softmax(logits, dim=-1)
    token_log_probs = torch.gather(log_probs, dim=2, index=sequence.unsqueeze(-1))
    total_log_prob = token_log_probs.sum()  # âœ… å¸¦æ¢¯åº¦

    return total_log_prob

def _compute_grpo_loss_with_recompute(self, all_inputs, all_sequences, all_rewards):
    """âœ… é‡æ–°è®¡ç®— log_probs å¹¶è®¡ç®— GRPO loss"""
    for inputs, sequences, rewards in zip(...):
        for sequence, advantage in zip(sequences, advantages):
            # âœ… é‡æ–°è®¡ç®—å¸¦æ¢¯åº¦çš„ log_prob
            log_prob = self._compute_sequence_log_probs(inputs, sequence)

            # âœ… GRPO lossï¼ˆå¸¦æ¢¯åº¦ï¼‰
            loss_term = -log_prob * advantage
            total_loss = total_loss + loss_term

    return total_loss  # âœ… å¯ä»¥åå‘ä¼ æ’­
```

**å…³é”®å˜åŒ–**ï¼š
1. é‡‡æ ·æ—¶åªä¿å­˜ token åºåˆ—ï¼Œä¸è®¡ç®— log_probs
2. æ–°å¢ `_compute_sequence_log_probs`ï¼šé‡æ–°å‰å‘ä¼ æ’­è®¡ç®—å¸¦æ¢¯åº¦çš„ log_probs
3. GRPO loss åŸºäºå¸¦æ¢¯åº¦çš„ log_probsï¼Œå¯ä»¥æ­£ç¡®æ›´æ–°å‚æ•°

### ä¿®å¤ 2ï¼šVL æ¨¡å‹æ­£ç¡®è¾“å…¥å›¾åƒ

**ä¿®æ”¹å‰**ï¼š
```python
def _sample_outputs(self, prompt: str, k: int):
    # âŒ åªæœ‰æ–‡æœ¬
    inputs = self.tokenizer(text=prompt, return_tensors="pt")

    for _ in range(k):
        # âŒ æ¨¡å‹çœ‹ä¸åˆ°å›¾åƒ
        generated = model.generate(**inputs, ...)
```

**ä¿®æ”¹å**ï¼š
```python
def _prepare_inputs(self, image: np.ndarray, prompt: str):
    """âœ… å‡†å¤‡ VL æ¨¡å‹è¾“å…¥ï¼ˆå›¾åƒ + æ–‡æœ¬ï¼‰"""
    from PIL import Image as PILImage

    if self.is_vl_model:
        pil_image = PILImage.fromarray(image)

        # âœ… æ„å»º VL æ¶ˆæ¯æ ¼å¼
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": pil_image},  # âœ… å›¾åƒ
                    {"type": "text", "text": prompt}        # âœ… æ–‡æœ¬
                ]
            }
        ]

        # âœ… ä½¿ç”¨ processor å¤„ç†
        text = self.tokenizer.apply_chat_template(messages, ...)
        inputs = self.tokenizer(
            text=[text],
            images=[pil_image],  # âœ… ä¼ å…¥å›¾åƒ
            return_tensors="pt"
        )

    return inputs  # âœ… åŒ…å«å›¾åƒç‰¹å¾ï¼ˆpixel_valuesï¼‰

def _sample_outputs(self, inputs: Dict, k: int):
    # âœ… inputs åŒ…å«å›¾åƒå’Œæ–‡æœ¬
    for _ in range(k):
        generated = model.generate(**inputs, ...)  # âœ… æ¨¡å‹èƒ½çœ‹åˆ°å›¾åƒ

def _compute_sequence_log_probs(self, inputs, sequence):
    # âœ… é‡æ–°è®¡ç®—æ—¶ä¹Ÿä¼ å…¥å›¾åƒç‰¹å¾
    if "pixel_values" in inputs:
        forward_inputs["pixel_values"] = inputs["pixel_values"]
    if "image_grid_thw" in inputs:
        forward_inputs["image_grid_thw"] = inputs["image_grid_thw"]

    outputs = model(**forward_inputs)  # âœ… å‰å‘ä¼ æ’­æ—¶åŒ…å«å›¾åƒ
```

**å…³é”®å˜åŒ–**ï¼š
1. æ–°å¢ `_prepare_inputs`ï¼šä½¿ç”¨ processor æ­£ç¡®å¤„ç†å›¾åƒå’Œæ–‡æœ¬
2. é‡‡æ ·æ—¶ä¼ å…¥å®Œæ•´çš„ inputsï¼ˆåŒ…å«å›¾åƒç‰¹å¾ï¼‰
3. é‡æ–°è®¡ç®— log_probs æ—¶ä¹Ÿä¼ å…¥å›¾åƒç‰¹å¾

---

## ğŸ“Š ä¿®å¤å¯¹æ¯”

### è®­ç»ƒæµç¨‹å¯¹æ¯”

| é˜¶æ®µ | ä¿®æ”¹å‰ | ä¿®æ”¹å |
|------|--------|--------|
| è¾“å…¥å‡†å¤‡ | âŒ åªæœ‰æ–‡æœ¬ | âœ… å›¾åƒ + æ–‡æœ¬ |
| é‡‡æ ·ç”Ÿæˆ | âŒ åœ¨ no_grad ä¸­è®¡ç®— log_probs | âœ… åªç”Ÿæˆåºåˆ— |
| å¥–åŠ±è®¡ç®— | âœ… æ­£ç¡® | âœ… æ­£ç¡® |
| Log_probs | âŒ æ— æ¢¯åº¦ | âœ… é‡æ–°è®¡ç®—ï¼Œå¸¦æ¢¯åº¦ |
| Loss è®¡ç®— | âŒ æ— æ³•åå‘ä¼ æ’­ | âœ… å¯ä»¥åå‘ä¼ æ’­ |
| å‚æ•°æ›´æ–° | âŒ ä¸æ›´æ–° | âœ… æ­£ç¡®æ›´æ–° LoRA |

### ä»£ç ç»“æ„å¯¹æ¯”

**ä¿®æ”¹å‰**ï¼š
```python
train_step():
    for each sample:
        outputs, log_probs = _sample_outputs(prompt, K)  # âŒ æ— æ¢¯åº¦
        rewards = compute_rewards(outputs)

    loss = compute_loss(log_probs, rewards)  # âŒ log_probs æ— æ¢¯åº¦
    loss.backward()  # âŒ æ— æ•ˆ
```

**ä¿®æ”¹å**ï¼š
```python
train_step():
    for each sample:
        inputs = _prepare_inputs(image, prompt)  # âœ… å›¾åƒ+æ–‡æœ¬
        outputs, sequences = _sample_outputs(inputs, K)  # âœ… åªç”Ÿæˆ
        rewards = compute_rewards(outputs)

    loss = _compute_grpo_loss_with_recompute(
        inputs, sequences, rewards
    )  # âœ… é‡æ–°è®¡ç®—å¸¦æ¢¯åº¦çš„ log_probs

    loss.backward()  # âœ… æ­£ç¡®æ›´æ–° LoRA
```

---

## ğŸ§ª å¦‚ä½•éªŒè¯ä¿®å¤

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š

```bash
python test_grpo_gradient.py
```

æµ‹è¯•ä¼šéªŒè¯ï¼š

### Test 1: æ¢¯åº¦æµåŠ¨
- âœ… LoRA å‚æ•°æ˜¯å¦ trainable
- âœ… æ‰§è¡Œ training step åå‚æ•°æ˜¯å¦æ›´æ–°
- âœ… å‚æ•°å˜åŒ–æ˜¯å¦å¤§äºé˜ˆå€¼

### Test 2: VL è¾“å…¥
- âœ… è¾“å…¥æ˜¯å¦åŒ…å« `pixel_values`ï¼ˆå›¾åƒç‰¹å¾ï¼‰
- âœ… è¾“å…¥æ˜¯å¦åŒ…å« `input_ids`ï¼ˆæ–‡æœ¬ï¼‰
- âœ… æ¨¡å‹æ˜¯å¦èƒ½åŒæ—¶çœ‹åˆ°å›¾åƒå’Œæ–‡æœ¬

**æœŸæœ›è¾“å‡º**ï¼š
```
Test 1: Gradient Flow
âœ“ Trainer initialized
âœ“ Found 128 trainable LoRA parameters
âœ“ Parameter 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight' updated
âœ… Gradient flow test PASSED

Test 2: Vision-Language Input
Is VL model: True
âœ“ 'input_ids' present, shape: torch.Size([1, 128])
âœ“ 'pixel_values' present, shape: torch.Size([1, 3, 448, 448])
âœ“ 'image_grid_thw' present, shape: torch.Size([1, 3])
âœ… Vision-Language input test PASSED

ğŸ‰ All tests PASSED!
```

---

## ğŸš€ ä¸‹ä¸€æ­¥

### 1. è¿è¡ŒéªŒè¯æµ‹è¯•ï¼ˆæ¨èï¼‰

åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼š
```bash
python test_grpo_gradient.py
```

ç¡®ä¿ä¸¤ä¸ªæµ‹è¯•éƒ½é€šè¿‡ã€‚

### 2. å¼€å§‹çœŸå®è®­ç»ƒ

```bash
bash scripts/train_negative_points.sh
```

ç°åœ¨è®­ç»ƒåº”è¯¥èƒ½ï¼š
- âœ… LoRA å‚æ•°ä¼šæ­£ç¡®æ›´æ–°
- âœ… æ¨¡å‹èƒ½çœ‹åˆ°å›¾åƒ
- âœ… èƒ½å­¦ä¹ é¢„æµ‹æœ‰æ•ˆçš„è´Ÿç‚¹

### 3. ç›‘æ§è®­ç»ƒæŒ‡æ ‡

è§‚å¯Ÿï¼š
- `loss` åº”è¯¥ä¸‹é™
- `mean_reward` åº”è¯¥ä¸Šå‡
- è´Ÿç‚¹åº”è¯¥é€æ¸è½åœ¨æ··æ·†åŒºåŸŸï¼ˆè€Œä¸æ˜¯ GT å†…ï¼‰

---

## ğŸ“š å‚è€ƒ

### GRPO ç®—æ³•æ ¸å¿ƒæ€æƒ³

Group Relative Policy Optimization (GRPO)ï¼š
1. å¯¹æ¯ä¸ªè¾“å…¥é‡‡æ · K ä¸ªè¾“å‡º
2. è®¡ç®—æ¯ä¸ªè¾“å‡ºçš„å¥–åŠ±
3. ç”¨**ç»„å†…ç›¸å¯¹å¥–åŠ±**ï¼ˆç›¸å¯¹äºç»„å†…å¹³å‡ï¼‰è®¡ç®—ä¼˜åŠ¿
4. ä¼˜åŒ– log P(output | input) Ã— advantage

**å…³é”®**ï¼šé‡æ–°è®¡ç®— log_probs æ—¶å¿…é¡»å¸¦æ¢¯åº¦ï¼

### Qwen2.5-VL è¾“å…¥æ ¼å¼

- å¿…é¡»ä½¿ç”¨ `AutoProcessor`ï¼ˆè€Œä¸æ˜¯ `AutoTokenizer`ï¼‰
- å›¾åƒé€šè¿‡ `pixel_values` ä¼ å…¥
- æ–‡æœ¬é€šè¿‡ chat template æ ¼å¼åŒ–
- `apply_chat_template` + `processor(text=..., images=...)` æ˜¯æ ‡å‡†ç”¨æ³•

---

## ğŸ’¡ æ€»ç»“

**åŸå§‹å®ç°çš„é—®é¢˜**ï¼š
- ğŸ”´ æ¢¯åº¦æ–­äº† â†’ LoRA ä¸æ›´æ–° â†’ æ¨¡å‹ä¸å­¦ä¹ 
- ğŸ”´ æ²¡å›¾åƒ â†’ æ¨¡å‹ç›²æ‰“ â†’ æ— æ³•é¢„æµ‹æœ‰æ•ˆè´Ÿç‚¹

**ä¿®å¤å**ï¼š
- âœ… æ¢¯åº¦æ­£ç¡®æµåŠ¨ â†’ LoRA æ­£ç¡®æ›´æ–°
- âœ… å›¾åƒæ­£ç¡®è¾“å…¥ â†’ æ¨¡å‹èƒ½çœ‹åˆ°è§†è§‰ä¿¡æ¯
- âœ… GRPO ç®—æ³•æ­£ç¡®å®ç° â†’ æ¨¡å‹èƒ½å­¦ä¹ 

ç°åœ¨å¯ä»¥çœŸæ­£å¼€å§‹è®­ç»ƒäº†ï¼ğŸ‰
