# Seg-Zero++: è´Ÿç‚¹é¢„æµ‹ä¸å¯¹æ¯”å¥–åŠ±çš„è¯¦ç»†å®æ–½æ–¹æ¡ˆ

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

**ç›®æ ‡**: åœ¨Seg-Zero-7BåŸºç¡€ä¸Šï¼Œé€šè¿‡å¼•å…¥è´Ÿç‚¹é¢„æµ‹æœºåˆ¶å’Œå¯¹æ¯”å¥–åŠ±å‡½æ•°ï¼Œæå‡ReasonSegå’ŒRefCOCO benchmarkçš„æ€§èƒ½ã€‚

**æ ¸å¿ƒåˆ›æ–°**: è®­ç»ƒMLLMåŒæ—¶é¢„æµ‹æ­£ç‚¹å’Œè´Ÿç‚¹ï¼Œåˆ©ç”¨SAM2çš„è´Ÿç‚¹promptingèƒ½åŠ›æ¥æ’é™¤è§†è§‰ä¸Šç›¸ä¼¼çš„èƒŒæ™¯åŒºåŸŸã€‚

**é¢„æœŸæå‡**: ReasonSeg gIoUä»57.5æå‡åˆ°60+ï¼ˆç›®æ ‡è¶…è¶ŠRSVPçš„60.3ï¼‰

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¾“å…¥ (Image + Query)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Qwen2.5-VL-7B (Reasoning Model)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  <think>                                                 â”‚    â”‚
â”‚  â”‚  æ¨ç†è¿‡ç¨‹...è¯†åˆ«ç›®æ ‡å¯¹è±¡...æ’é™¤å¹²æ‰°å¯¹è±¡...              â”‚    â”‚
â”‚  â”‚  </think>                                                â”‚    â”‚
â”‚  â”‚  <answer>                                                â”‚    â”‚
â”‚  â”‚  { "bbox": [x1,y1,x2,y2],                               â”‚    â”‚
â”‚  â”‚    "points_pos": [[px1,py1], [px2,py2]],     â† æ­£ç‚¹     â”‚    â”‚
â”‚  â”‚    "points_neg": [[nx1,ny1], [nx2,ny2]] }    â† è´Ÿç‚¹(æ–°) â”‚    â”‚
â”‚  â”‚  </answer>                                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SAM2.1-Large (Frozen)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Inputs:                                                 â”‚    â”‚
â”‚  â”‚  - bbox: [x1, y1, x2, y2]                               â”‚    â”‚
â”‚  â”‚  - point_coords: [[px1,py1], [px2,py2], [nx1,ny1], ...]â”‚    â”‚
â”‚  â”‚  - point_labels: [1, 1, 0, 0]  (1=æ­£, 0=è´Ÿ)             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Output: Segmentation Mask                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒæ­å»ºä¸ä»£ç å‡†å¤‡

### 1.1 å…‹éš†å¹¶è®¾ç½®Seg-Zeroç¯å¢ƒ

```bash
# å…‹éš†åŸå§‹Seg-Zeroä»“åº“
git clone https://github.com/dvlab-research/Seg-Zero.git
cd Seg-Zero

# å›é€€åˆ°å•ç›®æ ‡ç‰ˆæœ¬ï¼ˆå¦‚æœéœ€è¦ï¼‰
git reset --hard 77f9ea5887ec7e6abf398ed3cb483c65631c82b7

# åˆ›å»ºcondaç¯å¢ƒ
conda create -n segzero_plus python=3.12
conda activate segzero_plus

# å®‰è£…ä¾èµ–
pip install torch==2.6.0 torchvision==0.21.0
pip install -e .
```

### 1.2 ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹

```bash
mkdir -p pretrained_models
cd pretrained_models

# ä¸‹è½½Seg-Zero-7B checkpoint
git lfs install
git clone https://huggingface.co/Ricky06662/Seg-Zero-7B

# ä¸‹è½½SAM2.1-Large
# ä» https://github.com/facebookresearch/sam2 è·å–
wget https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt
```

### 1.3 ä¸‹è½½è®­ç»ƒæ•°æ®

```bash
# ä¸‹è½½RefCOCOg-9Kè®­ç»ƒæ•°æ®
python training_scripts/download_dataset.py

# æ•°æ®å°†ä¿å­˜åœ¨ ./data/refcocog_9k_840/
```

---

## ğŸ“ ç¬¬äºŒæ­¥ï¼šä¿®æ”¹è¾“å‡ºæ ¼å¼

### 2.1 æ–°çš„ç”¨æˆ·Promptæ¨¡æ¿

åˆ›å»ºæ–‡ä»¶ `verl/utils/reward_score/prompt_templates.py`:

```python
# åŸå§‹Seg-Zero Prompt
ORIGINAL_PROMPT = """
Please find '{Question}' with bbox and points.
Compare the difference between objects and find the most closely matched one.
Output the thinking process in <think> </think> and final answer in <answer> </answer> tags.
Output the one bbox and center points of two largest inscribed circles inside the interested object in JSON format.
i.e., <think> thinking process here </think>
<answer> { 'bbox': [10,100,200,210], 'points_1': [30,110], 'points_2': [35,180] } </answer>
"""

# æ–°çš„å¸¦è´Ÿç‚¹çš„Prompt
NEGATIVE_POINT_PROMPT = """
Please find '{Question}' with bbox, positive points, and negative points.
Compare the difference between objects and find the most closely matched one.
Identify confusing background regions that should be EXCLUDED from the segmentation.

Output the thinking process in <think> </think> and final answer in <answer> </answer> tags.

Output format in JSON:
- bbox: bounding box of the target object [x1, y1, x2, y2]
- points_pos: two positive points inside the target object [[x1,y1], [x2,y2]]
- points_neg: 1-3 negative points on confusing background regions [[x1,y1], ...] 
  (regions that look similar to target but should NOT be segmented)

Example:
<think> 
The query asks for "the person wearing red". 
There are two people in the image - one wearing red (target) and one wearing orange (similar, confusing).
I will place positive points on the person in red, and negative points on the orange clothing to help distinguish them.
</think>
<answer> {{ "bbox": [10,100,200,210], "points_pos": [[30,110], [35,180]], "points_neg": [[250,150]] }} </answer>
"""
```

### 2.2 ä¿®æ”¹è¾“å‡ºè§£æå™¨

åˆ›å»ºæ–‡ä»¶ `verl/utils/reward_score/output_parser.py`:

```python
import re
import json
from typing import Dict, List, Tuple, Optional

def parse_seg_zero_output(response: str) -> Dict:
    """
    è§£æSeg-Zeroæ¨¡å‹è¾“å‡ºï¼Œæ”¯æŒæ­£ç‚¹å’Œè´Ÿç‚¹æ ¼å¼
    
    Returns:
        {
            'think': str,           # æ¨ç†è¿‡ç¨‹
            'bbox': [x1,y1,x2,y2],  # è¾¹ç•Œæ¡†
            'points_pos': [[x,y], [x,y]],  # æ­£ç‚¹åˆ—è¡¨
            'points_neg': [[x,y], ...],    # è´Ÿç‚¹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            'format_valid': bool    # æ ¼å¼æ˜¯å¦æœ‰æ•ˆ
        }
    """
    result = {
        'think': '',
        'bbox': None,
        'points_pos': [],
        'points_neg': [],
        'format_valid': False
    }
    
    # æå–thinkéƒ¨åˆ†
    think_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)
    if think_match:
        result['think'] = think_match.group(1).strip()
    
    # æå–answeréƒ¨åˆ†
    answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
    if not answer_match:
        return result
    
    answer_text = answer_match.group(1).strip()
    
    try:
        # å°è¯•è§£æJSON
        # å¤„ç†å•å¼•å·çš„æƒ…å†µ
        answer_text = answer_text.replace("'", '"')
        data = json.loads(answer_text)
        
        # è§£æbbox
        if 'bbox' in data:
            result['bbox'] = data['bbox']
        
        # è§£ææ­£ç‚¹ - æ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼
        if 'points_pos' in data:
            result['points_pos'] = data['points_pos']
        elif 'points_1' in data and 'points_2' in data:
            # å…¼å®¹æ—§æ ¼å¼
            result['points_pos'] = [data['points_1'], data['points_2']]
        
        # è§£æè´Ÿç‚¹
        if 'points_neg' in data:
            result['points_neg'] = data['points_neg']
        
        # éªŒè¯æ ¼å¼
        result['format_valid'] = (
            result['bbox'] is not None and
            len(result['bbox']) == 4 and
            len(result['points_pos']) >= 1
        )
        
    except json.JSONDecodeError:
        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æ­£åˆ™åŒ¹é…
        bbox_match = re.search(r'"bbox"\s*:\s*\[(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\]', answer_text)
        if bbox_match:
            result['bbox'] = [int(x) for x in bbox_match.groups()]
        
        # æ­£åˆ™åŒ¹é…æ­£ç‚¹
        pos_match = re.search(r'"points_pos"\s*:\s*\[\[(\d+),\s*(\d+)\]', answer_text)
        if pos_match:
            result['points_pos'].append([int(pos_match.group(1)), int(pos_match.group(2))])
        
        # æ­£åˆ™åŒ¹é…è´Ÿç‚¹
        neg_matches = re.findall(r'"points_neg"\s*:\s*\[((?:\[\d+,\s*\d+\],?\s*)+)\]', answer_text)
        if neg_matches:
            for match in neg_matches:
                coords = re.findall(r'\[(\d+),\s*(\d+)\]', match)
                for coord in coords:
                    result['points_neg'].append([int(coord[0]), int(coord[1])])
    
    return result


def prepare_sam_prompts(parsed_output: Dict, image_size: Tuple[int, int] = (840, 840)) -> Dict:
    """
    å°†è§£æçš„è¾“å‡ºè½¬æ¢ä¸ºSAM2çš„è¾“å…¥æ ¼å¼
    
    Args:
        parsed_output: parse_seg_zero_outputçš„è¿”å›å€¼
        image_size: å›¾åƒå°ºå¯¸ (height, width)
    
    Returns:
        {
            'box': np.array([x1,y1,x2,y2]),
            'point_coords': np.array([[x,y], ...]),
            'point_labels': np.array([1,1,0,0,...])  # 1=æ­£, 0=è´Ÿ
        }
    """
    import numpy as np
    
    result = {
        'box': None,
        'point_coords': None,
        'point_labels': None
    }
    
    if parsed_output['bbox']:
        result['box'] = np.array(parsed_output['bbox'], dtype=np.float32)
    
    # åˆå¹¶æ­£ç‚¹å’Œè´Ÿç‚¹
    all_points = []
    all_labels = []
    
    for pt in parsed_output['points_pos']:
        all_points.append(pt)
        all_labels.append(1)  # æ­£ç‚¹æ ‡ç­¾
    
    for pt in parsed_output['points_neg']:
        all_points.append(pt)
        all_labels.append(0)  # è´Ÿç‚¹æ ‡ç­¾
    
    if all_points:
        result['point_coords'] = np.array(all_points, dtype=np.float32)
        result['point_labels'] = np.array(all_labels, dtype=np.int32)
    
    return result
```

---

## ğŸ¯ ç¬¬ä¸‰æ­¥ï¼šå®ç°å¥–åŠ±å‡½æ•°

### 3.1 å®Œæ•´å¥–åŠ±å‡½æ•°å®ç°

åˆ›å»ºæ–‡ä»¶ `verl/utils/reward_score/segmentation_rewards.py`:

```python
"""
Seg-Zero++ å¥–åŠ±å‡½æ•°æ¨¡å—
åŒ…å«æ ¼å¼å¥–åŠ±ã€ç²¾åº¦å¥–åŠ±å’Œå¯¹æ¯”å¥–åŠ±
"""

import numpy as np
from typing import Dict, List, Tuple, Optional
import torch

# ============================================
# æ ¼å¼å¥–åŠ±
# ============================================

def compute_think_format_reward(response: str) -> float:
    """
    æ£€æŸ¥æ˜¯å¦åŒ…å«æ­£ç¡®çš„<think></think>æ ‡ç­¾
    
    Returns:
        1.0 å¦‚æœæ ¼å¼æ­£ç¡®ï¼Œ0.0 å¦åˆ™
    """
    import re
    pattern = r'<think>.*?</think>'
    match = re.search(pattern, response, re.DOTALL)
    return 1.0 if match else 0.0


def compute_seg_format_reward_soft(parsed_output: Dict) -> float:
    """
    è½¯æ ¼å¼å¥–åŠ±ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„å…³é”®å­—
    
    Returns:
        1.0 å¦‚æœæ ¼å¼åŸºæœ¬æ­£ç¡®ï¼Œ0.0 å¦åˆ™
    """
    has_bbox = parsed_output['bbox'] is not None and len(parsed_output['bbox']) == 4
    has_points = len(parsed_output['points_pos']) >= 1
    
    return 1.0 if (has_bbox and has_points) else 0.0


def compute_seg_format_reward_strict(parsed_output: Dict) -> float:
    """
    ä¸¥æ ¼æ ¼å¼å¥–åŠ±ï¼šæ£€æŸ¥æ˜¯å¦å®Œå…¨ç¬¦åˆé¢„å®šä¹‰æ ¼å¼
    åŒ…æ‹¬æ­£ç‚¹å’Œè´Ÿç‚¹æ ¼å¼
    
    Returns:
        1.0 å¦‚æœæ ¼å¼å®Œå…¨æ­£ç¡®ï¼Œ0.0 å¦åˆ™
    """
    # æ£€æŸ¥bbox
    if parsed_output['bbox'] is None or len(parsed_output['bbox']) != 4:
        return 0.0
    
    # æ£€æŸ¥æ­£ç‚¹ï¼ˆè‡³å°‘2ä¸ªï¼‰
    if len(parsed_output['points_pos']) < 2:
        return 0.0
    
    # æ£€æŸ¥æ‰€æœ‰åæ ‡å€¼æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å­—
    try:
        bbox = parsed_output['bbox']
        if not all(isinstance(x, (int, float)) and 0 <= x <= 840 for x in bbox):
            return 0.0
        
        for pt in parsed_output['points_pos']:
            if not (len(pt) == 2 and all(isinstance(x, (int, float)) and 0 <= x <= 840 for x in pt)):
                return 0.0
        
        for pt in parsed_output['points_neg']:
            if not (len(pt) == 2 and all(isinstance(x, (int, float)) and 0 <= x <= 840 for x in pt)):
                return 0.0
                
    except (TypeError, ValueError):
        return 0.0
    
    return 1.0


# ============================================
# ç²¾åº¦å¥–åŠ±
# ============================================

def compute_bbox_iou_reward(
    pred_bbox: List[float],
    gt_bbox: List[float],
    threshold: float = 0.5
) -> float:
    """
    è®¡ç®—bbox IoUå¥–åŠ±ï¼ˆç¡¬å¥–åŠ±ï¼‰
    
    Args:
        pred_bbox: [x1, y1, x2, y2] é¢„æµ‹æ¡†
        gt_bbox: [x1, y1, x2, y2] çœŸå®æ¡†
        threshold: IoUé˜ˆå€¼
    
    Returns:
        1.0 å¦‚æœIoU > thresholdï¼Œ0.0 å¦åˆ™
    """
    if pred_bbox is None or gt_bbox is None:
        return 0.0
    
    # è®¡ç®—äº¤é›†
    x1 = max(pred_bbox[0], gt_bbox[0])
    y1 = max(pred_bbox[1], gt_bbox[1])
    x2 = min(pred_bbox[2], gt_bbox[2])
    y2 = min(pred_bbox[3], gt_bbox[3])
    
    if x2 <= x1 or y2 <= y1:
        return 0.0
    
    intersection = (x2 - x1) * (y2 - y1)
    
    # è®¡ç®—å¹¶é›†
    pred_area = (pred_bbox[2] - pred_bbox[0]) * (pred_bbox[3] - pred_bbox[1])
    gt_area = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])
    union = pred_area + gt_area - intersection
    
    iou = intersection / union if union > 0 else 0.0
    
    return 1.0 if iou > threshold else 0.0


def compute_bbox_l1_reward(
    pred_bbox: List[float],
    gt_bbox: List[float],
    threshold: float = 10.0
) -> float:
    """
    è®¡ç®—bbox L1è·ç¦»å¥–åŠ±
    
    Returns:
        1.0 å¦‚æœå¹³å‡L1è·ç¦» < thresholdï¼Œ0.0 å¦åˆ™
    """
    if pred_bbox is None or gt_bbox is None:
        return 0.0
    
    l1_dist = sum(abs(p - g) for p, g in zip(pred_bbox, gt_bbox)) / 4.0
    
    return 1.0 if l1_dist < threshold else 0.0


def compute_point_l1_reward(
    pred_points: List[List[float]],
    gt_mask: np.ndarray,
    threshold: float = 100.0
) -> float:
    """
    è®¡ç®—æ­£ç‚¹L1è·ç¦»å¥–åŠ±
    æ£€æŸ¥é¢„æµ‹çš„ç‚¹æ˜¯å¦åœ¨GT maskå†…éƒ¨
    
    Args:
        pred_points: é¢„æµ‹çš„æ­£ç‚¹åˆ—è¡¨ [[x1,y1], [x2,y2], ...]
        gt_mask: çœŸå®mask (H, W) äºŒå€¼æ•°ç»„
        threshold: è·ç¦»é˜ˆå€¼ï¼ˆåƒç´ ï¼‰
    
    Returns:
        1.0 å¦‚æœæ‰€æœ‰ç‚¹éƒ½åœ¨maskå†…æˆ–æ¥è¿‘ï¼Œ0.0 å¦åˆ™
    """
    if not pred_points or gt_mask is None:
        return 0.0
    
    h, w = gt_mask.shape
    
    for pt in pred_points:
        x, y = int(pt[0]), int(pt[1])
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å›¾åƒèŒƒå›´å†…
        if not (0 <= x < w and 0 <= y < h):
            return 0.0
        
        # æ£€æŸ¥æ˜¯å¦åœ¨maskå†…
        if gt_mask[y, x] > 0:
            continue
        
        # å¦‚æœä¸åœ¨maskå†…ï¼Œè®¡ç®—åˆ°maskçš„æœ€å°è·ç¦»
        mask_coords = np.argwhere(gt_mask > 0)  # (N, 2) in (y, x) format
        if len(mask_coords) == 0:
            return 0.0
        
        distances = np.sqrt(
            (mask_coords[:, 1] - x) ** 2 + 
            (mask_coords[:, 0] - y) ** 2
        )
        min_dist = distances.min()
        
        if min_dist > threshold:
            return 0.0
    
    return 1.0


# ============================================
# å¯¹æ¯”å¥–åŠ±ï¼ˆè´Ÿç‚¹å¥–åŠ±ï¼‰- æ ¸å¿ƒåˆ›æ–°
# ============================================

def compute_negative_point_reward(
    pred_neg_points: List[List[float]],
    gt_mask: np.ndarray,
    pred_bbox: List[float],
    confused_regions: Optional[np.ndarray] = None,
    alpha: float = 1.0,
    beta: float = 0.5
) -> float:
    """
    è®¡ç®—è´Ÿç‚¹å¯¹æ¯”å¥–åŠ±
    
    è®¾è®¡åŸåˆ™ï¼š
    1. è´Ÿç‚¹ä¸åº”è¯¥è½åœ¨GT maskå†…éƒ¨ï¼ˆæƒ©ç½šï¼‰
    2. è´Ÿç‚¹åº”è¯¥è½åœ¨"æ··æ·†åŒºåŸŸ"ï¼ˆå¥–åŠ±ï¼‰
    3. è´Ÿç‚¹åº”è¯¥åœ¨bboxé™„è¿‘ä½†ä¸åœ¨maskå†…
    
    Args:
        pred_neg_points: é¢„æµ‹çš„è´Ÿç‚¹åˆ—è¡¨
        gt_mask: çœŸå®mask
        pred_bbox: é¢„æµ‹çš„bbox
        confused_regions: å¯é€‰ï¼Œæ··æ·†åŒºåŸŸmaskï¼ˆSAMå¤šmaskæ­§ä¹‰åŒºåŸŸï¼‰
        alpha: æƒ©ç½šç³»æ•°ï¼ˆè´Ÿç‚¹è½åœ¨GTå†…ï¼‰
        beta: å¥–åŠ±ç³»æ•°ï¼ˆè´Ÿç‚¹è½åœ¨æ··æ·†åŒºåŸŸï¼‰
    
    Returns:
        å¥–åŠ±åˆ†æ•° [0.0, 1.0]
    """
    if not pred_neg_points:
        # æ²¡æœ‰é¢„æµ‹è´Ÿç‚¹ï¼Œç»™äºˆåŸºç¡€åˆ†
        return 0.5
    
    if gt_mask is None:
        return 0.0
    
    h, w = gt_mask.shape
    total_reward = 0.0
    valid_points = 0
    
    for pt in pred_neg_points:
        x, y = int(pt[0]), int(pt[1])
        
        # æ£€æŸ¥è¾¹ç•Œ
        if not (0 <= x < w and 0 <= y < h):
            continue
        
        valid_points += 1
        point_reward = 0.0
        
        # æƒ©ç½šï¼šè´Ÿç‚¹åœ¨GT maskå†…éƒ¨
        if gt_mask[y, x] > 0:
            point_reward -= alpha
        else:
            # å¥–åŠ±ï¼šè´Ÿç‚¹åœ¨maskå¤–éƒ¨
            point_reward += 0.3
        
        # å¥–åŠ±ï¼šè´Ÿç‚¹åœ¨æ··æ·†åŒºåŸŸ
        if confused_regions is not None and confused_regions[y, x] > 0:
            point_reward += beta
        
        # å¥–åŠ±ï¼šè´Ÿç‚¹åœ¨bboxé™„è¿‘ï¼ˆæœ‰æ•ˆçš„æ’é™¤åŒºåŸŸï¼‰
        if pred_bbox is not None:
            bx1, by1, bx2, by2 = pred_bbox
            # æ‰©å±•bboxåŒºåŸŸ
            margin = 50  # åƒç´ 
            extended_bbox = [
                max(0, bx1 - margin),
                max(0, by1 - margin),
                min(w, bx2 + margin),
                min(h, by2 + margin)
            ]
            if (extended_bbox[0] <= x <= extended_bbox[2] and 
                extended_bbox[1] <= y <= extended_bbox[3]):
                point_reward += 0.2
        
        total_reward += point_reward
    
    if valid_points == 0:
        return 0.0
    
    # å½’ä¸€åŒ–åˆ°[0, 1]
    avg_reward = total_reward / valid_points
    # å°†[-alpha, 0.5+beta]æ˜ å°„åˆ°[0, 1]
    normalized = (avg_reward + alpha) / (alpha + 0.5 + beta)
    
    return max(0.0, min(1.0, normalized))


def identify_confused_regions(
    image: np.ndarray,
    gt_mask: np.ndarray,
    sam_predictor,
    num_samples: int = 5
) -> np.ndarray:
    """
    ä½¿ç”¨SAMè¯†åˆ«æ··æ·†åŒºåŸŸ
    é€šè¿‡åœ¨ä¸åŒä½ç½®é‡‡æ ·ç‚¹ï¼Œæ‰¾åˆ°SAMè®¤ä¸ºå¯èƒ½æ˜¯ç›®æ ‡çš„åŒºåŸŸ
    
    Args:
        image: è¾“å…¥å›¾åƒ (H, W, 3)
        gt_mask: çœŸå®mask
        sam_predictor: SAM2 predictorå®ä¾‹
        num_samples: é‡‡æ ·æ¬¡æ•°
    
    Returns:
        confused_regions: æ··æ·†åŒºåŸŸmask (H, W)
    """
    h, w = gt_mask.shape
    confused_regions = np.zeros((h, w), dtype=np.float32)
    
    # è·å–GT maskçš„è¾¹ç•Œæ¡†
    mask_coords = np.argwhere(gt_mask > 0)
    if len(mask_coords) == 0:
        return confused_regions
    
    y_min, x_min = mask_coords.min(axis=0)
    y_max, x_max = mask_coords.max(axis=0)
    
    # åœ¨GT bboxå‘¨å›´é‡‡æ ·ç‚¹
    margin = 100
    sample_region = [
        max(0, x_min - margin),
        max(0, y_min - margin),
        min(w, x_max + margin),
        min(h, y_max + margin)
    ]
    
    for _ in range(num_samples):
        # éšæœºé‡‡æ ·ä¸€ä¸ªç‚¹ï¼ˆä¸åœ¨GT maskå†…ï¼‰
        for _ in range(10):  # æœ€å¤šå°è¯•10æ¬¡
            x = np.random.randint(sample_region[0], sample_region[2])
            y = np.random.randint(sample_region[1], sample_region[3])
            if gt_mask[y, x] == 0:
                break
        else:
            continue
        
        # ä½¿ç”¨SAMé¢„æµ‹
        sam_predictor.set_image(image)
        masks, scores, _ = sam_predictor.predict(
            point_coords=np.array([[x, y]]),
            point_labels=np.array([1]),
            multimask_output=True
        )
        
        # å°†SAMé¢„æµ‹çš„åŒºåŸŸï¼ˆéGTï¼‰åŠ å…¥æ··æ·†åŒºåŸŸ
        for mask, score in zip(masks, scores):
            if score > 0.5:  # åªè€ƒè™‘é«˜ç½®ä¿¡åº¦é¢„æµ‹
                # æ’é™¤ä¸GTé‡å çš„éƒ¨åˆ†
                non_gt_region = mask & (gt_mask == 0)
                confused_regions += non_gt_region.astype(np.float32)
    
    # å½’ä¸€åŒ–
    if confused_regions.max() > 0:
        confused_regions = confused_regions / confused_regions.max()
    
    return confused_regions


# ============================================
# æ€»å¥–åŠ±è®¡ç®—
# ============================================

def compute_total_reward(
    response: str,
    parsed_output: Dict,
    gt_bbox: List[float],
    gt_mask: np.ndarray,
    confused_regions: Optional[np.ndarray] = None,
    use_strict_format: bool = True,
    use_negative_reward: bool = True,
    weights: Dict[str, float] = None
) -> Dict[str, float]:
    """
    è®¡ç®—æ€»å¥–åŠ±
    
    Args:
        response: æ¨¡å‹åŸå§‹è¾“å‡º
        parsed_output: è§£æåçš„è¾“å‡º
        gt_bbox: çœŸå®bbox
        gt_mask: çœŸå®mask
        confused_regions: æ··æ·†åŒºåŸŸï¼ˆå¯é€‰ï¼‰
        use_strict_format: æ˜¯å¦ä½¿ç”¨ä¸¥æ ¼æ ¼å¼æ£€æŸ¥
        use_negative_reward: æ˜¯å¦ä½¿ç”¨è´Ÿç‚¹å¥–åŠ±
        weights: å„å¥–åŠ±é¡¹æƒé‡
    
    Returns:
        {
            'total': float,           # æ€»å¥–åŠ±
            'think_format': float,    # æ€è€ƒæ ¼å¼å¥–åŠ±
            'seg_format': float,      # åˆ†å‰²æ ¼å¼å¥–åŠ±
            'bbox_iou': float,        # bbox IoUå¥–åŠ±
            'bbox_l1': float,         # bbox L1å¥–åŠ±
            'point_l1': float,        # æ­£ç‚¹L1å¥–åŠ±
            'negative_point': float,  # è´Ÿç‚¹å¥–åŠ±
        }
    """
    default_weights = {
        'think_format': 1.0,
        'seg_format': 1.0,
        'bbox_iou': 1.0,
        'bbox_l1': 1.0,
        'point_l1': 1.0,
        'negative_point': 1.0  # æ–°å¢è´Ÿç‚¹å¥–åŠ±æƒé‡
    }
    
    if weights:
        default_weights.update(weights)
    weights = default_weights
    
    rewards = {}
    
    # æ ¼å¼å¥–åŠ±
    rewards['think_format'] = compute_think_format_reward(response)
    
    if use_strict_format:
        rewards['seg_format'] = compute_seg_format_reward_strict(parsed_output)
    else:
        rewards['seg_format'] = compute_seg_format_reward_soft(parsed_output)
    
    # ç²¾åº¦å¥–åŠ±
    rewards['bbox_iou'] = compute_bbox_iou_reward(parsed_output['bbox'], gt_bbox)
    rewards['bbox_l1'] = compute_bbox_l1_reward(parsed_output['bbox'], gt_bbox)
    rewards['point_l1'] = compute_point_l1_reward(parsed_output['points_pos'], gt_mask)
    
    # è´Ÿç‚¹å¥–åŠ±
    if use_negative_reward:
        rewards['negative_point'] = compute_negative_point_reward(
            parsed_output['points_neg'],
            gt_mask,
            parsed_output['bbox'],
            confused_regions
        )
    else:
        rewards['negative_point'] = 0.0
    
    # è®¡ç®—åŠ æƒæ€»å¥–åŠ±
    rewards['total'] = sum(
        rewards[key] * weights[key] 
        for key in weights.keys()
    )
    
    return rewards
```

### 3.2 å¥–åŠ±å‡½æ•°é›†æˆåˆ°GRPOè®­ç»ƒ

ä¿®æ”¹ `verl/trainer/fsdp_sft_trainer.py` æˆ–åˆ›å»ºæ–°çš„reward wrapper:

```python
# verl/utils/reward_score/reward_manager.py

from typing import Dict, List
import numpy as np
from .output_parser import parse_seg_zero_output, prepare_sam_prompts
from .segmentation_rewards import compute_total_reward, identify_confused_regions

class SegZeroRewardManager:
    """
    Seg-Zero++ å¥–åŠ±ç®¡ç†å™¨
    ç®¡ç†å¥–åŠ±è®¡ç®—ã€SAMæ¨ç†å’Œæ··æ·†åŒºåŸŸè¯†åˆ«
    """
    
    def __init__(
        self,
        sam_model_path: str = "pretrained_models/sam2.1_hiera_large.pt",
        use_negative_reward: bool = True,
        use_confused_regions: bool = True,
        device: str = "cuda"
    ):
        self.use_negative_reward = use_negative_reward
        self.use_confused_regions = use_confused_regions
        self.device = device
        
        # åŠ è½½SAM2æ¨¡å‹ï¼ˆç”¨äºè®¡ç®—maskå’Œæ··æ·†åŒºåŸŸï¼‰
        self._load_sam_model(sam_model_path)
    
    def _load_sam_model(self, model_path: str):
        """åŠ è½½SAM2æ¨¡å‹"""
        from sam2.build_sam import build_sam2
        from sam2.sam2_image_predictor import SAM2ImagePredictor
        
        self.sam_model = build_sam2(
            "sam2_hiera_l.yaml",
            model_path,
            device=self.device
        )
        self.sam_predictor = SAM2ImagePredictor(self.sam_model)
    
    def compute_reward_batch(
        self,
        responses: List[str],
        images: List[np.ndarray],
        gt_bboxes: List[List[float]],
        gt_masks: List[np.ndarray]
    ) -> List[Dict[str, float]]:
        """
        æ‰¹é‡è®¡ç®—å¥–åŠ±
        
        Args:
            responses: æ¨¡å‹è¾“å‡ºåˆ—è¡¨
            images: å›¾åƒåˆ—è¡¨ (N, H, W, 3)
            gt_bboxes: GT bboxåˆ—è¡¨
            gt_masks: GT maskåˆ—è¡¨
        
        Returns:
            å¥–åŠ±å­—å…¸åˆ—è¡¨
        """
        all_rewards = []
        
        for response, image, gt_bbox, gt_mask in zip(
            responses, images, gt_bboxes, gt_masks
        ):
            # è§£æè¾“å‡º
            parsed = parse_seg_zero_output(response)
            
            # è®¡ç®—æ··æ·†åŒºåŸŸï¼ˆå¯é€‰ï¼‰
            confused_regions = None
            if self.use_confused_regions and self.use_negative_reward:
                confused_regions = identify_confused_regions(
                    image, gt_mask, self.sam_predictor
                )
            
            # è®¡ç®—å¥–åŠ±
            rewards = compute_total_reward(
                response=response,
                parsed_output=parsed,
                gt_bbox=gt_bbox,
                gt_mask=gt_mask,
                confused_regions=confused_regions,
                use_negative_reward=self.use_negative_reward
            )
            
            all_rewards.append(rewards)
        
        return all_rewards
    
    def compute_segmentation_mask(
        self,
        image: np.ndarray,
        parsed_output: Dict
    ) -> np.ndarray:
        """
        ä½¿ç”¨SAM2è®¡ç®—åˆ†å‰²mask
        
        Args:
            image: è¾“å…¥å›¾åƒ
            parsed_output: è§£æåçš„æ¨¡å‹è¾“å‡º
        
        Returns:
            åˆ†å‰²mask
        """
        sam_prompts = prepare_sam_prompts(parsed_output)
        
        self.sam_predictor.set_image(image)
        
        masks, scores, _ = self.sam_predictor.predict(
            point_coords=sam_prompts['point_coords'],
            point_labels=sam_prompts['point_labels'],
            box=sam_prompts['box'],
            multimask_output=False
        )
        
        return masks[0]  # è¿”å›æœ€ä½³mask
```

---

## âš™ï¸ ç¬¬å››æ­¥ï¼šä¿®æ”¹è®­ç»ƒé…ç½®

### 4.1 åˆ›å»ºæ–°çš„è®­ç»ƒè„šæœ¬

åˆ›å»ºæ–‡ä»¶ `training_scripts/run_segzero_plus_7b.sh`:

```bash
#!/bin/bash

# Seg-Zero++ è®­ç»ƒè„šæœ¬
# åŸºäºSeg-Zero-7B checkpointï¼Œæ·»åŠ è´Ÿç‚¹é¢„æµ‹

export CUDA_VISIBLE_DEVICES=0,1
export WANDB_PROJECT="segzero-plus"

# åŸºç¡€é…ç½®
BASE_MODEL="pretrained_models/Seg-Zero-7B"
OUTPUT_DIR="outputs/segzero_plus_7b"
DATA_DIR="data/refcocog_9k_840"

# è®­ç»ƒå‚æ•°
BATCH_SIZE=2
GRAD_ACCUM=8
NUM_SAMPLES=8  # GRPOé‡‡æ ·æ•°
LR=1e-6
KL_COEF=5e-3
NUM_STEPS=300

# æ–°å¢ï¼šè´Ÿç‚¹å¥–åŠ±é…ç½®
USE_NEGATIVE_REWARD=true
NEGATIVE_REWARD_WEIGHT=1.0

python -m verl.trainer.main \
    trainer.project_name=$WANDB_PROJECT \
    trainer.experiment_name="segzero_plus_negative_points" \
    trainer.total_training_steps=$NUM_STEPS \
    \
    data.train_files=$DATA_DIR/train.parquet \
    data.val_files=$DATA_DIR/val.parquet \
    data.prompt_key="prompt" \
    data.image_key="image" \
    \
    actor_rollout_ref.model.path=$BASE_MODEL \
    actor_rollout_ref.actor.optim.lr=$LR \
    actor_rollout_ref.actor.ppo_mini_batch_size=$BATCH_SIZE \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 \
    actor_rollout_ref.actor.use_kl_loss=true \
    actor_rollout_ref.actor.kl_loss_coef=$KL_COEF \
    actor_rollout_ref.actor.fsdp.torch_dtype=bf16 \
    \
    actor_rollout_ref.rollout.n=$NUM_SAMPLES \
    actor_rollout_ref.rollout.temperature=1.0 \
    actor_rollout_ref.rollout.tensor_parallel_size=2 \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.85 \
    \
    algorithm.norm_adv_by_std_in_grpo=true \
    \
    custom.use_negative_reward=$USE_NEGATIVE_REWARD \
    custom.negative_reward_weight=$NEGATIVE_REWARD_WEIGHT \
    custom.use_strict_format=true \
    custom.prompt_template="negative_point" \
    \
    trainer.save_freq=50 \
    trainer.save_path=$OUTPUT_DIR
```

### 4.2 æ˜¾å­˜ä¼˜åŒ–é…ç½®ï¼ˆ2Ã—A6000ï¼‰

```bash
# é’ˆå¯¹2Ã—A6000 48GBä¼˜åŒ–çš„é…ç½®
# training_scripts/run_segzero_plus_7b_2xa6000.sh

#!/bin/bash

export CUDA_VISIBLE_DEVICES=0,1

# æ˜¾å­˜ä¼˜åŒ–å‚æ•°
MICRO_BATCH=1
GRAD_ACCUM=16  # æœ‰æ•ˆbatch size = 1 * 16 * 2 = 32
NUM_SAMPLES=4   # å‡å°‘GRPOé‡‡æ ·æ•°ä»¥èŠ‚çœæ˜¾å­˜
GPU_MEM_UTIL=0.90

python -m verl.trainer.main \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=$MICRO_BATCH \
    actor_rollout_ref.actor.gradient_checkpointing=true \
    actor_rollout_ref.rollout.n=$NUM_SAMPLES \
    actor_rollout_ref.rollout.tensor_parallel_size=2 \
    actor_rollout_ref.rollout.gpu_memory_utilization=$GPU_MEM_UTIL \
    # ... å…¶ä»–å‚æ•°åŒä¸Š
```

---

## ğŸ“Š ç¬¬äº”æ­¥ï¼šæ•°æ®é¢„å¤„ç†

### 5.1 ç”Ÿæˆæ··æ·†åŒºåŸŸæ ‡æ³¨

åˆ›å»ºæ–‡ä»¶ `prepare_dataset/generate_confused_regions.py`:

```python
"""
é¢„è®¡ç®—æ··æ·†åŒºåŸŸï¼ŒåŠ é€Ÿè®­ç»ƒ
"""

import os
import json
import numpy as np
from tqdm import tqdm
from PIL import Image
import pyarrow.parquet as pq
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

def generate_confused_regions_dataset(
    input_parquet: str,
    output_dir: str,
    sam_model_path: str,
    num_samples: int = 5
):
    """
    ä¸ºæ•°æ®é›†é¢„è®¡ç®—æ··æ·†åŒºåŸŸ
    """
    # åŠ è½½SAMæ¨¡å‹
    sam_model = build_sam2("sam2_hiera_l.yaml", sam_model_path, device="cuda")
    sam_predictor = SAM2ImagePredictor(sam_model)
    
    # è¯»å–æ•°æ®
    df = pq.read_table(input_parquet).to_pandas()
    
    os.makedirs(output_dir, exist_ok=True)
    
    confused_regions_data = []
    
    for idx, row in tqdm(df.iterrows(), total=len(df)):
        image = np.array(Image.open(row['image_path']))
        gt_mask = np.array(Image.open(row['mask_path']))
        
        # è®¡ç®—æ··æ·†åŒºåŸŸ
        confused = identify_confused_regions_fast(
            image, gt_mask, sam_predictor, num_samples
        )
        
        # ä¿å­˜æ··æ·†åŒºåŸŸmask
        confused_path = os.path.join(output_dir, f"confused_{idx:06d}.npy")
        np.save(confused_path, confused)
        
        confused_regions_data.append({
            'image_id': row.get('image_id', idx),
            'confused_region_path': confused_path
        })
    
    # ä¿å­˜ç´¢å¼•
    with open(os.path.join(output_dir, 'index.json'), 'w') as f:
        json.dump(confused_regions_data, f)
    
    print(f"Generated confused regions for {len(df)} samples")


def identify_confused_regions_fast(
    image: np.ndarray,
    gt_mask: np.ndarray,
    sam_predictor,
    num_samples: int = 5
) -> np.ndarray:
    """å¿«é€Ÿç‰ˆæ··æ·†åŒºåŸŸè¯†åˆ«"""
    h, w = gt_mask.shape[:2]
    confused = np.zeros((h, w), dtype=np.float32)
    
    # è·å–maskè¾¹ç•Œ
    mask_coords = np.argwhere(gt_mask > 0)
    if len(mask_coords) == 0:
        return confused
    
    y_min, x_min = mask_coords.min(axis=0)
    y_max, x_max = mask_coords.max(axis=0)
    
    # é‡‡æ ·åŒºåŸŸ
    margin = 80
    x_range = (max(0, x_min - margin), min(w, x_max + margin))
    y_range = (max(0, y_min - margin), min(h, y_max + margin))
    
    sam_predictor.set_image(image)
    
    for _ in range(num_samples):
        # åœ¨maskå¤–é‡‡æ ·
        for _ in range(5):
            x = np.random.randint(x_range[0], x_range[1])
            y = np.random.randint(y_range[0], y_range[1])
            if gt_mask[y, x] == 0:
                break
        else:
            continue
        
        masks, scores, _ = sam_predictor.predict(
            point_coords=np.array([[x, y]]),
            point_labels=np.array([1]),
            multimask_output=True
        )
        
        for mask, score in zip(masks, scores):
            if score > 0.5:
                non_gt = mask & (gt_mask == 0)
                confused += non_gt.astype(np.float32)
    
    if confused.max() > 0:
        confused /= confused.max()
    
    return confused


if __name__ == "__main__":
    generate_confused_regions_dataset(
        input_parquet="data/refcocog_9k_840/train.parquet",
        output_dir="data/refcocog_9k_840/confused_regions",
        sam_model_path="pretrained_models/sam2.1_hiera_large.pt"
    )
```

---

## ğŸ§ª ç¬¬å…­æ­¥ï¼šè¯„ä¼°è„šæœ¬

### 6.1 åˆ›å»ºè¯„ä¼°è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `evaluation_scripts/eval_segzero_plus.py`:

```python
"""
Seg-Zero++ è¯„ä¼°è„šæœ¬
æ”¯æŒReasonSegå’ŒRefCOCOè¯„ä¼°
"""

import os
import json
import argparse
import numpy as np
from tqdm import tqdm
from PIL import Image
import torch

from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from verl.utils.reward_score.output_parser import parse_seg_zero_output, prepare_sam_prompts
from verl.utils.reward_score.prompt_templates import NEGATIVE_POINT_PROMPT


def compute_iou(pred_mask: np.ndarray, gt_mask: np.ndarray) -> float:
    """è®¡ç®—IoU"""
    intersection = np.logical_and(pred_mask, gt_mask).sum()
    union = np.logical_or(pred_mask, gt_mask).sum()
    return intersection / union if union > 0 else 0.0


def compute_giou(pred_masks: list, gt_masks: list) -> float:
    """è®¡ç®—gIoU (å¹³å‡IoU)"""
    ious = [compute_iou(p, g) for p, g in zip(pred_masks, gt_masks)]
    return np.mean(ious)


def compute_ciou(pred_masks: list, gt_masks: list) -> float:
    """è®¡ç®—cIoU (ç´¯ç§¯IoU)"""
    total_intersection = sum(
        np.logical_and(p, g).sum() for p, g in zip(pred_masks, gt_masks)
    )
    total_union = sum(
        np.logical_or(p, g).sum() for p, g in zip(pred_masks, gt_masks)
    )
    return total_intersection / total_union if total_union > 0 else 0.0


class SegZeroPlusEvaluator:
    def __init__(
        self,
        model_path: str,
        sam_model_path: str,
        device: str = "cuda"
    ):
        self.device = device
        
        # åŠ è½½Qwen2.5-VLæ¨¡å‹
        print(f"Loading model from {model_path}")
        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.processor = AutoProcessor.from_pretrained(model_path)
        
        # åŠ è½½SAM2æ¨¡å‹
        print(f"Loading SAM2 from {sam_model_path}")
        self.sam_model = build_sam2("sam2_hiera_l.yaml", sam_model_path, device=device)
        self.sam_predictor = SAM2ImagePredictor(self.sam_model)
    
    def generate_response(self, image: Image.Image, query: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        prompt = NEGATIVE_POINT_PROMPT.format(Question=query)
        
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        inputs = self.processor(
            text=[text],
            images=[image],
            padding=True,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=False
            )
        
        response = self.processor.batch_decode(
            generated_ids[:, inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )[0]
        
        return response
    
    def predict_mask(self, image: np.ndarray, response: str) -> np.ndarray:
        """æ ¹æ®æ¨¡å‹å“åº”ç”Ÿæˆåˆ†å‰²mask"""
        parsed = parse_seg_zero_output(response)
        
        if not parsed['format_valid']:
            # è¿”å›ç©ºmask
            return np.zeros(image.shape[:2], dtype=bool)
        
        sam_prompts = prepare_sam_prompts(parsed)
        
        self.sam_predictor.set_image(image)
        
        masks, scores, _ = self.sam_predictor.predict(
            point_coords=sam_prompts['point_coords'],
            point_labels=sam_prompts['point_labels'],
            box=sam_prompts['box'],
            multimask_output=False
        )
        
        return masks[0]
    
    def evaluate_dataset(
        self,
        data_path: str,
        output_path: str = None,
        max_samples: int = None
    ) -> dict:
        """è¯„ä¼°æ•°æ®é›†"""
        # åŠ è½½æ•°æ®
        with open(data_path) as f:
            data = json.load(f)
        
        if max_samples:
            data = data[:max_samples]
        
        pred_masks = []
        gt_masks = []
        results = []
        
        for item in tqdm(data, desc="Evaluating"):
            # åŠ è½½å›¾åƒ
            image = Image.open(item['image_path']).convert('RGB')
            image_np = np.array(image)
            
            # åŠ è½½GT mask
            gt_mask = np.array(Image.open(item['mask_path'])) > 0
            
            # ç”Ÿæˆå“åº”
            response = self.generate_response(image, item['query'])
            
            # é¢„æµ‹mask
            pred_mask = self.predict_mask(image_np, response)
            
            # è®¡ç®—IoU
            iou = compute_iou(pred_mask, gt_mask)
            
            pred_masks.append(pred_mask)
            gt_masks.append(gt_mask)
            
            results.append({
                'image_id': item.get('image_id', ''),
                'query': item['query'],
                'iou': float(iou),
                'response': response
            })
        
        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        giou = compute_giou(pred_masks, gt_masks)
        ciou = compute_ciou(pred_masks, gt_masks)
        
        metrics = {
            'gIoU': float(giou),
            'cIoU': float(ciou),
            'num_samples': len(data)
        }
        
        print(f"\n=== Evaluation Results ===")
        print(f"gIoU: {giou:.4f}")
        print(f"cIoU: {ciou:.4f}")
        print(f"Samples: {len(data)}")
        
        # ä¿å­˜ç»“æœ
        if output_path:
            with open(output_path, 'w') as f:
                json.dump({
                    'metrics': metrics,
                    'results': results
                }, f, indent=2)
            print(f"Results saved to {output_path}")
        
        return metrics


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_path', type=str, required=True)
    parser.add_argument('--sam_model_path', type=str, 
                        default='pretrained_models/sam2.1_hiera_large.pt')
    parser.add_argument('--data_path', type=str, required=True)
    parser.add_argument('--output_path', type=str, default=None)
    parser.add_argument('--max_samples', type=int, default=None)
    args = parser.parse_args()
    
    evaluator = SegZeroPlusEvaluator(
        model_path=args.model_path,
        sam_model_path=args.sam_model_path
    )
    
    evaluator.evaluate_dataset(
        data_path=args.data_path,
        output_path=args.output_path,
        max_samples=args.max_samples
    )


if __name__ == '__main__':
    main()
```

### 6.2 è¯„ä¼°è„šæœ¬Shell wrapper

åˆ›å»ºæ–‡ä»¶ `evaluation_scripts/eval_reasonseg_segzero_plus.sh`:

```bash
#!/bin/bash

MODEL_PATH="outputs/segzero_plus_7b/checkpoint-300"
SAM_PATH="pretrained_models/sam2.1_hiera_large.pt"
DATA_PATH="data/ReasonSeg/test.json"
OUTPUT_PATH="results/reasonseg_test_results.json"

python evaluation_scripts/eval_segzero_plus.py \
    --model_path $MODEL_PATH \
    --sam_model_path $SAM_PATH \
    --data_path $DATA_PATH \
    --output_path $OUTPUT_PATH
```

---

## ğŸ“ˆ ç¬¬ä¸ƒæ­¥ï¼šæ¶ˆèå®éªŒè®¾è®¡

### 7.1 æ¶ˆèå®éªŒé…ç½®

```python
# experiments/ablation_configs.py

ABLATION_EXPERIMENTS = {
    # åŸºçº¿ï¼šåŸå§‹Seg-Zero
    "baseline": {
        "use_negative_reward": False,
        "use_confused_regions": False,
        "prompt_template": "original"
    },
    
    # æ¶ˆè1ï¼šä»…æ·»åŠ è´Ÿç‚¹è¾“å‡ºï¼ˆæ— è´Ÿç‚¹å¥–åŠ±ï¼‰
    "neg_output_only": {
        "use_negative_reward": False,
        "use_confused_regions": False,
        "prompt_template": "negative_point"
    },
    
    # æ¶ˆè2ï¼šè´Ÿç‚¹+ç®€å•å¥–åŠ±ï¼ˆæ— æ··æ·†åŒºåŸŸï¼‰
    "neg_simple_reward": {
        "use_negative_reward": True,
        "use_confused_regions": False,
        "prompt_template": "negative_point",
        "negative_reward_weight": 1.0
    },
    
    # æ¶ˆè3ï¼šè´Ÿç‚¹+å®Œæ•´å¯¹æ¯”å¥–åŠ±
    "neg_full_reward": {
        "use_negative_reward": True,
        "use_confused_regions": True,
        "prompt_template": "negative_point",
        "negative_reward_weight": 1.0
    },
    
    # æ¶ˆè4ï¼šä¸åŒè´Ÿç‚¹æ•°é‡
    "neg_1_point": {"max_negative_points": 1},
    "neg_2_points": {"max_negative_points": 2},
    "neg_3_points": {"max_negative_points": 3},
    
    # æ¶ˆè5ï¼šä¸åŒå¥–åŠ±æƒé‡
    "neg_weight_0.5": {"negative_reward_weight": 0.5},
    "neg_weight_1.0": {"negative_reward_weight": 1.0},
    "neg_weight_2.0": {"negative_reward_weight": 2.0},
}
```

### 7.2 é¢„æœŸç»“æœè¡¨æ ¼æ¨¡æ¿

```markdown
| Method | RefCOCOg | ReasonSeg | 
|--------|----------|-----------|
| Baseline (Seg-Zero-7B) | 74.2 | 57.5 |
| + Neg Output Only | ~74.5 | ~58.0 |
| + Simple Neg Reward | ~75.0 | ~59.0 |
| + Full Contrastive Reward | **76.0+** | **60.5+** |
```

---

## â° ç¬¬å…«æ­¥ï¼šæ—¶é—´çº¿è§„åˆ’

```
Week 1-2: ç¯å¢ƒæ­å»º & ä»£ç ä¿®æ”¹
â”œâ”€â”€ Day 1-2: ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½
â”œâ”€â”€ Day 3-5: ä¿®æ”¹è¾“å‡ºæ ¼å¼å’Œè§£æå™¨
â”œâ”€â”€ Day 6-10: å®ç°å¥–åŠ±å‡½æ•°
â””â”€â”€ Day 11-14: é›†æˆåˆ°è®­ç»ƒæµç¨‹

Week 3-4: åˆæ­¥å®éªŒ
â”œâ”€â”€ Day 15-18: å°è§„æ¨¡éªŒè¯å®éªŒï¼ˆ1K samplesï¼‰
â”œâ”€â”€ Day 19-21: è°ƒè¯•å’Œä¿®å¤bug
â””â”€â”€ Day 22-28: æ¶ˆèå®éªŒï¼ˆè´Ÿç‚¹æ•°é‡ã€æƒé‡ï¼‰

Week 5-6: ä¸»å®éªŒ
â”œâ”€â”€ Day 29-35: å®Œæ•´è®­ç»ƒï¼ˆ9K samples, ~3å¤©ï¼‰
â”œâ”€â”€ Day 36-38: è¯„ä¼°ReasonSeg
â””â”€â”€ Day 39-42: è¯„ä¼°RefCOCOç³»åˆ—

Week 7-8: åˆ†æä¸è®ºæ–‡
â”œâ”€â”€ Day 43-49: ç»“æœåˆ†æã€å¯è§†åŒ–
â””â”€â”€ Day 50-56: è®ºæ–‡æ’°å†™
```

---

## ğŸ”§ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: æ˜¾å­˜ä¸è¶³
```bash
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘batch sizeå’Œé‡‡æ ·æ•°
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1
actor_rollout_ref.rollout.n=4
actor_rollout_ref.actor.gradient_checkpointing=true
```

### Q2: è´Ÿç‚¹æ€»æ˜¯é¢„æµ‹åœ¨maskå†…éƒ¨
- å¢åŠ æƒ©ç½šç³»æ•° alpha
- æ£€æŸ¥æ··æ·†åŒºåŸŸç”Ÿæˆæ˜¯å¦æ­£å¸¸
- è€ƒè™‘æ·»åŠ hard constraint

### Q3: è®­ç»ƒä¸ç¨³å®š
- é™ä½å­¦ä¹ ç‡åˆ°5e-7
- å¢åŠ KLç³»æ•°åˆ°1e-2
- ä½¿ç”¨æ›´ä¿å®ˆçš„clipèŒƒå›´

### Q4: æ ¼å¼å¥–åŠ±ä¸æ”¶æ•›
- æ£€æŸ¥promptæ¨¡æ¿æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ç¤ºä¾‹
- ç¡®ä¿è§£æå™¨èƒ½æ­£ç¡®å¤„ç†å„ç§æ ¼å¼å˜ä½“
- è€ƒè™‘å…ˆç”¨SFTé¢„çƒ­å‡ æ­¥

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. Seg-Zero Paper: https://arxiv.org/abs/2503.06520
2. SAM2 Documentation: https://github.com/facebookresearch/sam2
3. veRL Framework: https://github.com/volcengine/verl
4. GRPO Algorithm: https://arxiv.org/abs/2402.03300
